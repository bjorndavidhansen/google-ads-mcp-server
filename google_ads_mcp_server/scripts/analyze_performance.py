#!/usr/bin/env python
"""
Performance Analysis Script

This script analyzes the performance profile results generated by the baseline tests
and identifies potential optimization targets based on execution time, memory usage,
and other metrics.

Usage:
    python analyze_performance.py [results_path]

Args:
    results_path: Optional path to the JSON results file. If not provided,
                 the script will use the most recent results file.
"""

import os
import sys
import json
import glob
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, Any, List, Optional

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def find_latest_results(profile_dir: str = None) -> str:
    """Find the most recent profile results file."""
    if profile_dir is None:
        profile_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "performance_profiles")
    
    # Look for summary files first
    summary_files = glob.glob(os.path.join(profile_dir, "summary_*.json"))
    if summary_files:
        return max(summary_files, key=os.path.getmtime)
    
    # Look for any results files
    result_files = glob.glob(os.path.join(profile_dir, "*_*.json"))
    if result_files:
        # Filter out detailed_ files
        result_files = [f for f in result_files if not os.path.basename(f).startswith("detailed_")]
        if result_files:
            return max(result_files, key=os.path.getmtime)
    
    raise FileNotFoundError(f"No performance results found in {profile_dir}")

def load_results(results_path: str) -> Dict[str, Any]:
    """Load performance results from a JSON file."""
    with open(results_path, 'r') as f:
        return json.load(f)

def generate_performance_report(results: Dict[str, Any], output_dir: str = None) -> str:
    """
    Generate a detailed performance report from the results.
    
    Args:
        results: Performance results data
        output_dir: Directory to save the report
        
    Returns:
        Path to the generated report
    """
    if output_dir is None:
        output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "performance_profiles")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = os.path.join(output_dir, f"performance_report_{timestamp}.md")
    
    # Sort tests by average execution time (slowest first)
    sorted_tests = sorted(
        [(name, data) for name, data in results.items()],
        key=lambda x: x[1].get('avg_execution_time', 0),
        reverse=True
    )
    
    with open(report_path, 'w') as f:
        f.write("# Google Ads MCP Server - Performance Analysis Report\n\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## Performance Summary\n\n")
        f.write("| Test | Avg Time (s) | Min Time (s) | Max Time (s) | Success Rate |\n")
        f.write("|------|-------------|-------------|-------------|-------------|\n")
        
        for name, data in sorted_tests:
            avg_time = data.get('avg_execution_time', 0)
            min_time = data.get('min_execution_time', 0)
            max_time = data.get('max_execution_time', 0)
            success_rate = data.get('success_rate', 0) * 100
            
            f.write(f"| {name} | {avg_time:.4f} | {min_time:.4f} | {max_time:.4f} | {success_rate:.1f}% |\n")
        
        f.write("\n## Optimization Targets\n\n")
        f.write("The following tests are identified as potential optimization targets based on execution time:\n\n")
        
        # Only consider tests with at least 1 second execution time for optimization
        optimization_targets = [
            (name, data) for name, data in sorted_tests 
            if data.get('avg_execution_time', 0) >= 1.0
        ]
        
        if optimization_targets:
            for i, (name, data) in enumerate(optimization_targets[:5], 1):
                avg_time = data.get('avg_execution_time', 0)
                f.write(f"{i}. **{name}** - {avg_time:.4f} seconds\n")
        else:
            f.write("No significant optimization targets identified. All tests execute in less than 1 second.\n")
        
        f.write("\n## Performance Classification\n\n")
        f.write("Tests are classified into the following categories:\n\n")
        f.write("- **Critical** (> 5 seconds): Severe performance bottlenecks requiring immediate attention\n")
        f.write("- **Slow** (2-5 seconds): Performance issues that should be addressed\n")
        f.write("- **Moderate** (1-2 seconds): Worth optimizing if time permits\n")
        f.write("- **Fast** (< 1 second): Acceptable performance\n\n")
        
        critical = []
        slow = []
        moderate = []
        fast = []
        
        for name, data in sorted_tests:
            avg_time = data.get('avg_execution_time', 0)
            if avg_time > 5:
                critical.append((name, avg_time))
            elif avg_time > 2:
                slow.append((name, avg_time))
            elif avg_time > 1:
                moderate.append((name, avg_time))
            else:
                fast.append((name, avg_time))
        
        f.write("### Critical Paths\n\n")
        if critical:
            for name, time in critical:
                f.write(f"- {name} ({time:.4f} seconds)\n")
        else:
            f.write("No critical performance issues found.\n")
        
        f.write("\n### Slow Paths\n\n")
        if slow:
            for name, time in slow:
                f.write(f"- {name} ({time:.4f} seconds)\n")
        else:
            f.write("No slow performance issues found.\n")
        
        f.write("\n### Moderate Paths\n\n")
        if moderate:
            for name, time in moderate:
                f.write(f"- {name} ({time:.4f} seconds)\n")
        else:
            f.write("No moderate performance issues found.\n")
            
        # Don't list all the fast paths if there are many
        if len(fast) <= 10:
            f.write("\n### Fast Paths\n\n")
            for name, time in fast:
                f.write(f"- {name} ({time:.4f} seconds)\n")
        else:
            f.write(f"\n### Fast Paths\n\n{len(fast)} tests are performing well (< 1 second).\n")
    
    print(f"Generated performance report: {report_path}")
    return report_path

def create_performance_charts(results: Dict[str, Any], output_dir: str = None) -> None:
    """Create performance charts from the results."""
    if output_dir is None:
        output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "performance_profiles")
    
    # Check if matplotlib is available
    try:
        import matplotlib.pyplot as plt
        import numpy as np
    except ImportError:
        print("Matplotlib not available. Skipping chart generation.")
        return
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    charts_dir = os.path.join(output_dir, "charts")
    os.makedirs(charts_dir, exist_ok=True)
    
    try:
        # Sort tests by average execution time
        sorted_tests = sorted(
            [(name, data) for name, data in results.items()],
            key=lambda x: x[1].get('avg_execution_time', 0),
            reverse=True
        )
        
        # Only take top 10 slowest tests for charts
        top_tests = sorted_tests[:10]
        
        # Create bar chart of execution times
        plt.figure(figsize=(12, 8))
        
        test_names = [name for name, _ in top_tests]
        avg_times = [data.get('avg_execution_time', 0) for _, data in top_tests]
        
        # Truncate long test names
        truncated_names = []
        for name in test_names:
            if len(name) > 30:
                truncated_names.append(name[:27] + "...")
            else:
                truncated_names.append(name)
        
        # Plot bars
        y_pos = np.arange(len(truncated_names))
        plt.barh(y_pos, avg_times, align='center', alpha=0.7)
        plt.yticks(y_pos, truncated_names)
        plt.xlabel('Average Execution Time (seconds)')
        plt.title('Top 10 Slowest Operations')
        plt.tight_layout()
        
        # Save chart
        chart_path = os.path.join(charts_dir, f"top_slowest_{timestamp}.png")
        plt.savefig(chart_path)
        plt.close()
        
        # Create a pie chart for performance categories
        plt.figure(figsize=(10, 10))
        
        critical = sum(1 for _, data in sorted_tests if data.get('avg_execution_time', 0) > 5)
        slow = sum(1 for _, data in sorted_tests if 2 < data.get('avg_execution_time', 0) <= 5)
        moderate = sum(1 for _, data in sorted_tests if 1 < data.get('avg_execution_time', 0) <= 2)
        fast = sum(1 for _, data in sorted_tests if data.get('avg_execution_time', 0) <= 1)
        
        labels = ['Critical (>5s)', 'Slow (2-5s)', 'Moderate (1-2s)', 'Fast (<1s)']
        sizes = [critical, slow, moderate, fast]
        colors = ['red', 'orange', 'yellow', 'green']
        explode = (0.1, 0.05, 0, 0)
        
        # Remove empty categories
        non_zero_indices = [i for i, size in enumerate(sizes) if size > 0]
        filtered_labels = [labels[i] for i in non_zero_indices]
        filtered_sizes = [sizes[i] for i in non_zero_indices]
        filtered_colors = [colors[i] for i in non_zero_indices]
        filtered_explode = [explode[i] for i in non_zero_indices]
        
        if filtered_sizes:  # Only create if there's data
            plt.pie(filtered_sizes, explode=filtered_explode, labels=filtered_labels, colors=filtered_colors,
                    autopct='%1.1f%%', shadow=True, startangle=90)
            plt.axis('equal')
            plt.title('Performance Category Distribution')
            
            # Save chart
            chart_path = os.path.join(charts_dir, f"performance_categories_{timestamp}.png")
            plt.savefig(chart_path)
            plt.close()
        
        print(f"Generated performance charts in: {charts_dir}")
        
    except Exception as e:
        print(f"Error generating charts: {e}")

def main():
    """Main entry point for the performance analysis script."""
    # Get path to results file from argument or find the latest
    if len(sys.argv) > 1:
        results_path = sys.argv[1]
    else:
        try:
            results_path = find_latest_results()
            print(f"Using most recent results file: {results_path}")
        except FileNotFoundError as e:
            print(f"Error: {e}")
            return 1
    
    try:
        # Load results
        results = load_results(results_path)
        
        # Generate report
        report_path = generate_performance_report(results)
        
        # Create charts
        create_performance_charts(results)
        
        print("\nAnalysis complete. Check the performance_profiles directory for results.")
        return 0
        
    except Exception as e:
        print(f"Error analyzing performance results: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 